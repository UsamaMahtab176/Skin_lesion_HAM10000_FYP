{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e302c6d8-eec7-4460-8685-828ca8d1e0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-24 18:46:50.805402: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-24 18:46:55.444727: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-08-24 18:46:55.446799: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-08-24 18:46:55.446824: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6551c91b-0a40-4706-9ff7-0ba2988bc11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metadata CSV file\n",
    "metadata = pd.read_csv('HAM10000_metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efa1da78-ff40-412c-a020-67058f2bdfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images and labels\n",
    "image_folder = 'HAM10000_Images'  # Replace with the actual image folder path\n",
    "# image_size = (224, 224)\n",
    "# image_size = (32, 32)\n",
    "image_size = (96,96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "320d7a57-f2ad-494c-a576-d16d52b11f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "labels = []\n",
    "\n",
    "for index, row in metadata.iterrows():\n",
    "    image_path = os.path.join(image_folder, row['image_id'] + '.jpg')\n",
    "    image = imread(image_path)\n",
    "    resized_image = resize(image, image_size)\n",
    "    images.append(resized_image)\n",
    "    labels.append(row['dx'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faf3429-7bde-426e-bf20-cbb05e7b2ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.array(images)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "051cb79a-1b06-4fd9-83a3-17b497e22de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to categorical format\n",
    "label_mapping = {label: idx for idx, label in enumerate(np.unique(labels))}\n",
    "labels_encoded = np.array([label_mapping[label] for label in labels])\n",
    "labels_categorical = to_categorical(labels_encoded, num_classes=len(label_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fba01de-6dbd-4471-96d5-e71acd1133c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the dataset into train, validation, and test sets\n",
    "train_images, test_images, train_labels, test_labels = train_test_split(\n",
    "    images, labels_categorical, test_size=0.2, random_state=42\n",
    ")\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(\n",
    "    train_images, train_labels, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4abcbd94-d42e-4905-aa64-6af7731beebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images shape: (6409, 96, 96, 3)\n",
      "Train labels shape: (6409, 7)\n",
      "Validation images shape: (1603, 96, 96, 3)\n",
      "Validation labels shape: (1603, 7)\n",
      "Test images shape: (2003, 96, 96, 3)\n",
      "Test labels shape: (2003, 7)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print the shape of each set\n",
    "print(\"Train images shape:\", train_images.shape)\n",
    "print(\"Train labels shape:\", train_labels.shape)\n",
    "print(\"Validation images shape:\", val_images.shape)\n",
    "print(\"Validation labels shape:\", val_labels.shape)\n",
    "print(\"Test images shape:\", test_images.shape)\n",
    "print(\"Test labels shape:\", test_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cc5c92e-1eea-4ee6-ab62-dcfe1764f76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an ImageDataGenerator instance for data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,  # Random rotation up to 20 degrees\n",
    "    width_shift_range=0.1,  # Random horizontal shift\n",
    "    height_shift_range=0.1,  # Random vertical shift\n",
    "    shear_range=0.2,  # Shear transformations\n",
    "    zoom_range=0.2,  # Random zoom\n",
    "    horizontal_flip=True,  # Random horizontal flips\n",
    "    vertical_flip=True,  # Random vertical flips\n",
    "    fill_mode='nearest'  # Fill points outside the input with the nearest pixel\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae71b4fd-203e-4b6d-80db-98d7d9dde571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply data augmentation only to the training set\n",
    "train_datagen = datagen.flow(train_images, train_labels, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "864906df-ac00-4d28-a271-df97f533ff3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-19 21:05:02.383518: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-08-19 21:05:02.383575: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-08-19 21:05:02.383617: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (instance-20230816-171017): /proc/driver/nvidia/version does not exist\n",
      "2023-08-19 21:05:02.385145: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained VGG16 model (excluding top layers and with ImageNet weights)\n",
    "base_model = VGG16(include_top=False, weights='imagenet', input_shape=(96, 96, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73d8e4a5-e46c-4694-a67f-a33a078585bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Freeze the layers of the base model to retain pre-trained weights\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0ba5744-750c-4cde-8df2-c0ac69abe980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add custom classification layers on top of the base model\n",
    "x = GlobalAveragePooling2D()(base_model.output)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "output = Dense(7, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7beef44-b483-4480-b4a9-1ed9038aaa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the final model\n",
    "model = Model(inputs=base_model.input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe956cbf-c671-403c-8383-40b99fa701dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9138492e-0c66-4ab9-9e91-7c7c74db48cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define early stopping criteria\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # Monitor the validation loss\n",
    "    patience=10,          # Number of epochs with no improvement after which training will be stopped\n",
    "    restore_best_weights=True  # Restore the best model weights based on validation loss\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e912f2b8-d812-4aaf-82ae-6014aa044002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "201/201 [==============================] - 394s 2s/step - loss: 1.3092 - accuracy: 0.6166 - val_loss: 1.0872 - val_accuracy: 0.6606\n",
      "Epoch 2/40\n",
      "201/201 [==============================] - 389s 2s/step - loss: 1.0483 - accuracy: 0.6725 - val_loss: 0.9925 - val_accuracy: 0.6656\n",
      "Epoch 3/40\n",
      "201/201 [==============================] - 389s 2s/step - loss: 0.9999 - accuracy: 0.6744 - val_loss: 0.9504 - val_accuracy: 0.6750\n",
      "Epoch 4/40\n",
      "201/201 [==============================] - 386s 2s/step - loss: 0.9739 - accuracy: 0.6789 - val_loss: 0.9274 - val_accuracy: 0.6831\n",
      "Epoch 5/40\n",
      "201/201 [==============================] - 390s 2s/step - loss: 0.9499 - accuracy: 0.6829 - val_loss: 0.9105 - val_accuracy: 0.6887\n",
      "Epoch 6/40\n",
      "201/201 [==============================] - 388s 2s/step - loss: 0.9314 - accuracy: 0.6897 - val_loss: 0.9025 - val_accuracy: 0.7006\n",
      "Epoch 7/40\n",
      "201/201 [==============================] - 388s 2s/step - loss: 0.9188 - accuracy: 0.6889 - val_loss: 0.8848 - val_accuracy: 0.7031\n",
      "Epoch 8/40\n",
      "201/201 [==============================] - 386s 2s/step - loss: 0.9068 - accuracy: 0.6915 - val_loss: 0.8741 - val_accuracy: 0.6974\n",
      "Epoch 9/40\n",
      "201/201 [==============================] - 386s 2s/step - loss: 0.8932 - accuracy: 0.6939 - val_loss: 0.8649 - val_accuracy: 0.7049\n",
      "Epoch 10/40\n",
      "201/201 [==============================] - 387s 2s/step - loss: 0.8835 - accuracy: 0.6961 - val_loss: 0.8614 - val_accuracy: 0.7031\n",
      "Epoch 11/40\n",
      "201/201 [==============================] - 387s 2s/step - loss: 0.8813 - accuracy: 0.6973 - val_loss: 0.8532 - val_accuracy: 0.7056\n",
      "Epoch 12/40\n",
      "201/201 [==============================] - 384s 2s/step - loss: 0.8695 - accuracy: 0.7003 - val_loss: 0.8511 - val_accuracy: 0.7168\n",
      "Epoch 13/40\n",
      "201/201 [==============================] - 385s 2s/step - loss: 0.8645 - accuracy: 0.7024 - val_loss: 0.8456 - val_accuracy: 0.7187\n",
      "Epoch 14/40\n",
      "201/201 [==============================] - 385s 2s/step - loss: 0.8587 - accuracy: 0.7054 - val_loss: 0.8458 - val_accuracy: 0.7180\n",
      "Epoch 15/40\n",
      "201/201 [==============================] - 382s 2s/step - loss: 0.8551 - accuracy: 0.7060 - val_loss: 0.8374 - val_accuracy: 0.7187\n",
      "Epoch 16/40\n",
      "201/201 [==============================] - 386s 2s/step - loss: 0.8481 - accuracy: 0.7084 - val_loss: 0.8344 - val_accuracy: 0.7224\n",
      "Epoch 17/40\n",
      "201/201 [==============================] - 385s 2s/step - loss: 0.8405 - accuracy: 0.7062 - val_loss: 0.8370 - val_accuracy: 0.7249\n",
      "Epoch 18/40\n",
      "201/201 [==============================] - 387s 2s/step - loss: 0.8357 - accuracy: 0.7120 - val_loss: 0.8273 - val_accuracy: 0.7193\n",
      "Epoch 19/40\n",
      "201/201 [==============================] - 388s 2s/step - loss: 0.8359 - accuracy: 0.7117 - val_loss: 0.8222 - val_accuracy: 0.7205\n",
      "Epoch 20/40\n",
      "201/201 [==============================] - 389s 2s/step - loss: 0.8317 - accuracy: 0.7124 - val_loss: 0.8295 - val_accuracy: 0.7293\n",
      "Epoch 21/40\n",
      "201/201 [==============================] - 387s 2s/step - loss: 0.8273 - accuracy: 0.7103 - val_loss: 0.8209 - val_accuracy: 0.7261\n",
      "Epoch 22/40\n",
      "201/201 [==============================] - 386s 2s/step - loss: 0.8229 - accuracy: 0.7156 - val_loss: 0.8217 - val_accuracy: 0.7187\n",
      "Epoch 23/40\n",
      "201/201 [==============================] - 387s 2s/step - loss: 0.8213 - accuracy: 0.7123 - val_loss: 0.8164 - val_accuracy: 0.7268\n",
      "Epoch 24/40\n",
      "201/201 [==============================] - 387s 2s/step - loss: 0.8169 - accuracy: 0.7160 - val_loss: 0.8151 - val_accuracy: 0.7199\n",
      "Epoch 25/40\n",
      "201/201 [==============================] - 389s 2s/step - loss: 0.8141 - accuracy: 0.7181 - val_loss: 0.8152 - val_accuracy: 0.7261\n",
      "Epoch 26/40\n",
      "201/201 [==============================] - 388s 2s/step - loss: 0.8114 - accuracy: 0.7188 - val_loss: 0.8089 - val_accuracy: 0.7243\n",
      "Epoch 27/40\n",
      "201/201 [==============================] - 388s 2s/step - loss: 0.8092 - accuracy: 0.7198 - val_loss: 0.8080 - val_accuracy: 0.7243\n",
      "Epoch 28/40\n",
      "201/201 [==============================] - 390s 2s/step - loss: 0.8062 - accuracy: 0.7204 - val_loss: 0.8098 - val_accuracy: 0.7205\n",
      "Epoch 29/40\n",
      "201/201 [==============================] - 389s 2s/step - loss: 0.8052 - accuracy: 0.7193 - val_loss: 0.8095 - val_accuracy: 0.7199\n",
      "Epoch 30/40\n",
      "201/201 [==============================] - 388s 2s/step - loss: 0.7988 - accuracy: 0.7218 - val_loss: 0.8020 - val_accuracy: 0.7218\n",
      "Epoch 31/40\n",
      "201/201 [==============================] - 389s 2s/step - loss: 0.7996 - accuracy: 0.7191 - val_loss: 0.8005 - val_accuracy: 0.7230\n",
      "Epoch 32/40\n",
      "201/201 [==============================] - 388s 2s/step - loss: 0.7969 - accuracy: 0.7220 - val_loss: 0.8024 - val_accuracy: 0.7205\n",
      "Epoch 33/40\n",
      "201/201 [==============================] - 387s 2s/step - loss: 0.7913 - accuracy: 0.7204 - val_loss: 0.7992 - val_accuracy: 0.7249\n",
      "Epoch 34/40\n",
      "201/201 [==============================] - 387s 2s/step - loss: 0.7925 - accuracy: 0.7255 - val_loss: 0.7966 - val_accuracy: 0.7236\n",
      "Epoch 35/40\n",
      "201/201 [==============================] - 386s 2s/step - loss: 0.7829 - accuracy: 0.7240 - val_loss: 0.7928 - val_accuracy: 0.7318\n",
      "Epoch 36/40\n",
      "201/201 [==============================] - 385s 2s/step - loss: 0.7863 - accuracy: 0.7223 - val_loss: 0.7945 - val_accuracy: 0.7236\n",
      "Epoch 37/40\n",
      "201/201 [==============================] - 385s 2s/step - loss: 0.7852 - accuracy: 0.7265 - val_loss: 0.7912 - val_accuracy: 0.7218\n",
      "Epoch 38/40\n",
      "201/201 [==============================] - 385s 2s/step - loss: 0.7865 - accuracy: 0.7263 - val_loss: 0.7943 - val_accuracy: 0.7218\n",
      "Epoch 39/40\n",
      "201/201 [==============================] - 386s 2s/step - loss: 0.7827 - accuracy: 0.7227 - val_loss: 0.7962 - val_accuracy: 0.7224\n",
      "Epoch 40/40\n",
      "201/201 [==============================] - 385s 2s/step - loss: 0.7790 - accuracy: 0.7277 - val_loss: 0.7903 - val_accuracy: 0.7274\n"
     ]
    }
   ],
   "source": [
    "# Train the model using the fit() method\n",
    "epochs = 40  # Number of training epochs\n",
    "batch_size = 32  # Number of samples in each batch\n",
    "\n",
    "history = model.fit(\n",
    "    train_datagen,  # If using data augmentation\n",
    "    # x=train_images, y=train_labels,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(val_images, val_labels),\n",
    "    callbacks=[early_stopping]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ccc83790-acb5-48fe-ace5-0ca8ef040210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/51 [==============================] - 76s 1s/step - loss: 0.7903 - accuracy: 0.7274\n",
      "Validation Loss: 0.7902973890304565\n",
      "Validation Accuracy: 0.7273861765861511\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation set\n",
    "val_loss, val_accuracy = model.evaluate(val_images, val_labels, batch_size=batch_size)\n",
    "\n",
    "print(\"Validation Loss:\", val_loss)\n",
    "print(\"Validation Accuracy:\", val_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "78b66331-f1e4-4568-a198-6a614aab39f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 95s 2s/step - loss: 0.8035 - accuracy: 0.7099\n",
      "Test Loss: 0.803539514541626\n",
      "Test Accuracy: 0.7099350690841675\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the testing set\n",
    "test_loss, test_accuracy = model.evaluate(test_images, test_labels, batch_size=batch_size)\n",
    "\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cfca724a-1d2a-418a-8bd5-bcdb9d4fd9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 14). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_96x96/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_96x96/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('model_96x96')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "67726c7a-c009-405b-afbe-fdde6348d03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'akiec': 0, 'bcc': 1, 'bkl': 2, 'df': 3, 'mel': 4, 'nv': 5, 'vasc': 6}\n"
     ]
    }
   ],
   "source": [
    "print(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1e208f-6161-43ea-a1e0-3051d8f69969",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m110",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m110"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
