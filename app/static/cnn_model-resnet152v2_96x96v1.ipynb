{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "papermill": {
     "duration": 7.94135,
     "end_time": "2023-07-25T17:43:11.676389",
     "exception": false,
     "start_time": "2023-07-25T17:43:03.735039",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-26 14:08:05.896744: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-26 14:08:08.370367: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-08-26 14:08:08.370798: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-08-26 14:08:08.370817: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import ResNet50, ResNet152V2  # Import ResNet152V2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Flatten, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "papermill": {
     "duration": 5.602342,
     "end_time": "2023-07-25T17:43:17.425772",
     "exception": false,
     "start_time": "2023-07-25T17:43:11.82343",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load metadata CSV file\n",
    "metadata = pd.read_csv('HAM10000_metadata.csv')\n",
    "\n",
    "# Load images and labels\n",
    "image_folder = 'HAM10000_Images'\n",
    "image_size = (96, 96)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "papermill": {
     "duration": 0.057441,
     "end_time": "2023-07-25T17:43:17.877032",
     "exception": false,
     "start_time": "2023-07-25T17:43:17.819591",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "images = []\n",
    "labels = []\n",
    "\n",
    "for index, row in metadata.iterrows():\n",
    "    image_path = os.path.join(image_folder, row['image_id'] + '.jpg')\n",
    "    image = imread(image_path)\n",
    "    resized_image = resize(image, image_size)\n",
    "    images.append(resized_image)\n",
    "    labels.append(row['dx'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "papermill": {
     "duration": 0.071325,
     "end_time": "2023-07-25T17:43:17.997358",
     "exception": false,
     "start_time": "2023-07-25T17:43:17.926033",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "images = np.array(images)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to categorical format\n",
    "label_mapping = {label: idx for idx, label in enumerate(np.unique(labels))}\n",
    "labels_encoded = np.array([label_mapping[label] for label in labels])\n",
    "labels_categorical = to_categorical(labels_encoded, num_classes=len(label_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train, validation, and test sets\n",
    "train_images, test_images, train_labels, test_labels = train_test_split(\n",
    "    images, labels_categorical, test_size=0.3, random_state=42\n",
    ")\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(\n",
    "    train_images, train_labels, test_size=0.3, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images shape: (4907, 96, 96, 3)\n",
      "Train labels shape: (4907, 7)\n",
      "Validation images shape: (2103, 96, 96, 3)\n",
      "Validation labels shape: (2103, 7)\n",
      "Test images shape: (3005, 96, 96, 3)\n",
      "Test labels shape: (3005, 7)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print the shape of each set\n",
    "print(\"Train images shape:\", train_images.shape)\n",
    "print(\"Train labels shape:\", train_labels.shape)\n",
    "print(\"Validation images shape:\", val_images.shape)\n",
    "print(\"Validation labels shape:\", val_labels.shape)\n",
    "print(\"Test images shape:\", test_images.shape)\n",
    "print(\"Test labels shape:\", test_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an ImageDataGenerator instance for data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=30,  # Increase rotation range\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply data augmentation only to the training set\n",
    "train_datagen = datagen.flow(train_images, train_labels, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Resnet152v2***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-26 14:24:47.828453: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-08-26 14:24:47.828613: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-08-26 14:24:47.828700: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (instance-20230816-171017): /proc/driver/nvidia/version does not exist\n",
      "2023-08-26 14:24:47.829367: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained ResNet152V2 model (excluding top layers and with ImageNet weights)\n",
    "base_model = ResNet152V2(include_top=False, weights='imagenet', input_shape=(96, 96, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the layers of the base model to retain pre-trained weights\n",
    "fine_tune_at = 555\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resnet152v2 (Functional)    (None, 3, 3, 2048)        58331648  \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 18432)             0         \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 18432)            73728     \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 128)               2359424   \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 128)              512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 7)                 903       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 60,766,215\n",
      "Trainable params: 2,397,447\n",
      "Non-trainable params: 58,368,768\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Implement your specified model\n",
    "model = Sequential()\n",
    "model.add(base_model)\n",
    "model.add(Flatten())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze the last block of layers in the base model for fine-tuning\n",
    "# fine_tune_at = -12\n",
    "# for layer in model.layers[:fine_tune_at]:\n",
    "#     layer.trainable = False\n",
    "# for layer in model.layers[fine_tune_at:]:\n",
    "#     layer.trainable = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the learning rate\n",
    "def lr_scheduler(epoch):\n",
    "    if epoch < 10:\n",
    "        return 0.0001\n",
    "    else:\n",
    "        return 0.0001 * np.exp(0.1 * (10 - epoch))\n",
    "\n",
    "lr_schedule = tf.keras.callbacks.LearningRateScheduler(lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "154/154 [==============================] - 296s 2s/step - loss: 2.3707 - accuracy: 0.3405 - val_loss: 1.4892 - val_accuracy: 0.5873 - lr: 1.0000e-04\n",
      "Epoch 2/50\n",
      "154/154 [==============================] - 277s 2s/step - loss: 1.9578 - accuracy: 0.4638 - val_loss: 1.3804 - val_accuracy: 0.6429 - lr: 1.0000e-04\n",
      "Epoch 3/50\n",
      "154/154 [==============================] - 282s 2s/step - loss: 1.7481 - accuracy: 0.5266 - val_loss: 1.2881 - val_accuracy: 0.6743 - lr: 1.0000e-04\n",
      "Epoch 4/50\n",
      "154/154 [==============================] - 286s 2s/step - loss: 1.6316 - accuracy: 0.5578 - val_loss: 1.2688 - val_accuracy: 0.6876 - lr: 1.0000e-04\n",
      "Epoch 5/50\n",
      "154/154 [==============================] - 278s 2s/step - loss: 1.5213 - accuracy: 0.5773 - val_loss: 1.1840 - val_accuracy: 0.7071 - lr: 1.0000e-04\n",
      "Epoch 6/50\n",
      "154/154 [==============================] - 279s 2s/step - loss: 1.4042 - accuracy: 0.6110 - val_loss: 1.1939 - val_accuracy: 0.7042 - lr: 1.0000e-04\n",
      "Epoch 7/50\n",
      "154/154 [==============================] - 279s 2s/step - loss: 1.3333 - accuracy: 0.6340 - val_loss: 1.1507 - val_accuracy: 0.7147 - lr: 1.0000e-04\n",
      "Epoch 8/50\n",
      "154/154 [==============================] - 278s 2s/step - loss: 1.3024 - accuracy: 0.6458 - val_loss: 1.1360 - val_accuracy: 0.7233 - lr: 1.0000e-04\n",
      "Epoch 9/50\n",
      "154/154 [==============================] - 282s 2s/step - loss: 1.2352 - accuracy: 0.6546 - val_loss: 1.1611 - val_accuracy: 0.7085 - lr: 1.0000e-04\n",
      "Epoch 10/50\n",
      "154/154 [==============================] - 281s 2s/step - loss: 1.2349 - accuracy: 0.6629 - val_loss: 1.1368 - val_accuracy: 0.7095 - lr: 1.0000e-04\n",
      "Epoch 11/50\n",
      "154/154 [==============================] - 280s 2s/step - loss: 1.1702 - accuracy: 0.6760 - val_loss: 1.1576 - val_accuracy: 0.7123 - lr: 1.0000e-04\n",
      "Epoch 12/50\n",
      "154/154 [==============================] - 278s 2s/step - loss: 1.1108 - accuracy: 0.6825 - val_loss: 1.1523 - val_accuracy: 0.7123 - lr: 9.0484e-05\n",
      "Epoch 13/50\n",
      "154/154 [==============================] - 279s 2s/step - loss: 1.0890 - accuracy: 0.6921 - val_loss: 1.1474 - val_accuracy: 0.7242 - lr: 8.1873e-05\n",
      "Epoch 14/50\n",
      "154/154 [==============================] - 279s 2s/step - loss: 1.0689 - accuracy: 0.6856 - val_loss: 1.2080 - val_accuracy: 0.7109 - lr: 7.4082e-05\n",
      "Epoch 15/50\n",
      "154/154 [==============================] - 280s 2s/step - loss: 1.0184 - accuracy: 0.7002 - val_loss: 1.1888 - val_accuracy: 0.7190 - lr: 6.7032e-05\n",
      "Epoch 16/50\n",
      "154/154 [==============================] - 282s 2s/step - loss: 1.0224 - accuracy: 0.6906 - val_loss: 1.2205 - val_accuracy: 0.7109 - lr: 6.0653e-05\n",
      "Epoch 17/50\n",
      "154/154 [==============================] - 278s 2s/step - loss: 1.0165 - accuracy: 0.6943 - val_loss: 1.2102 - val_accuracy: 0.7166 - lr: 5.4881e-05\n",
      "Epoch 18/50\n",
      "154/154 [==============================] - 279s 2s/step - loss: 0.9920 - accuracy: 0.6980 - val_loss: 1.1621 - val_accuracy: 0.7218 - lr: 4.9659e-05\n"
     ]
    }
   ],
   "source": [
    "# Train the model with the improved settings\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "\n",
    "history = model.fit(\n",
    "    train_datagen,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(val_images, val_labels),\n",
    "    callbacks=[early_stopping, lr_schedule],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 81s 1s/step - loss: 1.1360 - accuracy: 0.7233\n",
      "Validation Loss: 1.1360048055648804\n",
      "Validation Accuracy: 0.7232524752616882\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation set\n",
    "val_loss, val_accuracy = model.evaluate(val_images, val_labels, batch_size=batch_size)\n",
    "print(\"Validation Loss:\", val_loss)\n",
    "print(\"Validation Accuracy:\", val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 115s 1s/step - loss: 1.1205 - accuracy: 0.7012\n",
      "Test Loss: 1.1205350160598755\n",
      "Test Accuracy: 0.701164722442627\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the testing set\n",
    "test_loss, test_accuracy = model.evaluate(test_images, test_labels, batch_size=batch_size)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 156). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: cnn_model-resnet152v2_96x96v1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: cnn_model-resnet152v2_96x96v1/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('cnn_model-resnet152v2_96x96v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m110",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m110"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
