{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f91ab135-8433-489e-808f-077c35ea99a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-24 19:24:41.338750: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-24 19:24:45.754294: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-08-24 19:24:45.755585: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-08-24 19:24:45.755603: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92a511d5-eebc-4bc4-ab91-20d8fd0d5382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metadata CSV file\n",
    "metadata = pd.read_csv('HAM10000_metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13ff5d7f-c0a7-4bcf-89c9-ef4d0439860c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images and labels\n",
    "image_folder = 'HAM10000_Images'\n",
    "image_size = (96, 96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e76b0e0d-e2f1-4828-967d-549b8dd5e961",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "labels = []\n",
    "\n",
    "for index, row in metadata.iterrows():\n",
    "    image_path = os.path.join(image_folder, row['image_id'] + '.jpg')\n",
    "    image = imread(image_path)\n",
    "    resized_image = resize(image, image_size)\n",
    "    images.append(resized_image)\n",
    "    labels.append(row['dx'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de7e3502-01fa-48f1-ad6b-93abbc9f84ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.array(images)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15fa9881-3f5c-465a-9fe2-8f0a4188692d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to categorical format\n",
    "label_mapping = {label: idx for idx, label in enumerate(np.unique(labels))}\n",
    "labels_encoded = np.array([label_mapping[label] for label in labels])\n",
    "labels_categorical = to_categorical(labels_encoded, num_classes=len(label_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a2f8bda-7162-4824-8c8e-98d3ce6f2fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train, validation, and test sets\n",
    "train_images, test_images, train_labels, test_labels = train_test_split(\n",
    "    images, labels_categorical, test_size=0.2, random_state=42\n",
    ")\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(\n",
    "    train_images, train_labels, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "512aa9ba-0ab5-434e-a60a-9e692b47a49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images shape: (6409, 96, 96, 3)\n",
      "Train labels shape: (6409, 7)\n",
      "Validation images shape: (1603, 96, 96, 3)\n",
      "Validation labels shape: (1603, 7)\n",
      "Test images shape: (2003, 96, 96, 3)\n",
      "Test labels shape: (2003, 7)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print the shape of each set\n",
    "print(\"Train images shape:\", train_images.shape)\n",
    "print(\"Train labels shape:\", train_labels.shape)\n",
    "print(\"Validation images shape:\", val_images.shape)\n",
    "print(\"Validation labels shape:\", val_labels.shape)\n",
    "print(\"Test images shape:\", test_images.shape)\n",
    "print(\"Test labels shape:\", test_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "183c918f-6dda-42bc-8eec-3f599a22c057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an ImageDataGenerator instance for data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=30,  # Increase rotation range\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f35a4a6-6694-451a-bb3f-ab2c72ae8bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply data augmentation only to the training set\n",
    "train_datagen = datagen.flow(train_images, train_labels, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15bc795d-af34-44c9-b5ac-d3cc4412ccc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-24 19:40:28.738391: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-08-24 19:40:28.738445: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-08-24 19:40:28.738527: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (instance-20230816-171017): /proc/driver/nvidia/version does not exist\n",
      "2023-08-24 19:40:28.739969: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94765736/94765736 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained ResNet50 model (excluding top layers and with ImageNet weights)\n",
    "base_model = ResNet50(include_top=False, weights='imagenet', input_shape=(96, 96, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c76f224-4e1e-4a54-923b-e1180daa0141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the layers of the base model to retain pre-trained weights\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a99ced8-dce8-449f-98e8-1d252e4ff627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add custom classification layers on top of the base model\n",
    "x = GlobalAveragePooling2D()(base_model.output)\n",
    "x = Dense(256, activation='relu')(x)  # Increased dense layer size\n",
    "x = Dropout(0.5)(x)  # Increase dropout rate\n",
    "output = Dense(7, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b5091eb1-79be-4c28-9193-36a401117398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the final model\n",
    "model = Model(inputs=base_model.input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "13e9d364-f854-404e-9f1b-481f29eee61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze the last block of layers in the base model for fine-tuning\n",
    "for layer in base_model.layers[:-12]:\n",
    "    layer.trainable = False\n",
    "for layer in base_model.layers[-12:]:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15365e22-64da-4511-94e3-ea58ddd7c54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the learning rate\n",
    "def lr_scheduler(epoch):\n",
    "    if epoch < 10:\n",
    "        return 0.0001\n",
    "    else:\n",
    "        return 0.0001 * np.exp(0.1 * (10 - epoch))\n",
    "\n",
    "lr_schedule = tf.keras.callbacks.LearningRateScheduler(lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f11e60da-d98a-4819-9c4b-f8abe340ec63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0932bd00-8e41-42ca-913f-7c709ba91de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Early stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "70d418da-a8da-4e9f-b529-019edd8442fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "201/201 [==============================] - 179s 865ms/step - loss: 0.9385 - accuracy: 0.6730 - val_loss: 0.9122 - val_accuracy: 0.6900 - lr: 1.0000e-04\n",
      "Epoch 2/40\n",
      "201/201 [==============================] - 173s 863ms/step - loss: 0.8988 - accuracy: 0.6911 - val_loss: 0.8434 - val_accuracy: 0.7018 - lr: 1.0000e-04\n",
      "Epoch 3/40\n",
      "201/201 [==============================] - 174s 866ms/step - loss: 0.9007 - accuracy: 0.6776 - val_loss: 0.8659 - val_accuracy: 0.7056 - lr: 1.0000e-04\n",
      "Epoch 4/40\n",
      "201/201 [==============================] - 173s 861ms/step - loss: 0.8955 - accuracy: 0.6861 - val_loss: 0.8636 - val_accuracy: 0.6906 - lr: 1.0000e-04\n",
      "Epoch 5/40\n",
      "201/201 [==============================] - 172s 857ms/step - loss: 0.8872 - accuracy: 0.6831 - val_loss: 0.8846 - val_accuracy: 0.6931 - lr: 1.0000e-04\n",
      "Epoch 6/40\n",
      "201/201 [==============================] - 170s 846ms/step - loss: 0.8832 - accuracy: 0.6883 - val_loss: 0.8485 - val_accuracy: 0.6862 - lr: 1.0000e-04\n",
      "Epoch 7/40\n",
      "201/201 [==============================] - 175s 871ms/step - loss: 0.8913 - accuracy: 0.6907 - val_loss: 0.8391 - val_accuracy: 0.7099 - lr: 1.0000e-04\n",
      "Epoch 8/40\n",
      "201/201 [==============================] - 172s 856ms/step - loss: 0.8759 - accuracy: 0.6912 - val_loss: 0.8442 - val_accuracy: 0.6962 - lr: 1.0000e-04\n",
      "Epoch 9/40\n",
      "201/201 [==============================] - 176s 875ms/step - loss: 0.8847 - accuracy: 0.6854 - val_loss: 0.8509 - val_accuracy: 0.6949 - lr: 1.0000e-04\n",
      "Epoch 10/40\n",
      "201/201 [==============================] - 174s 866ms/step - loss: 0.8911 - accuracy: 0.6858 - val_loss: 0.8415 - val_accuracy: 0.6993 - lr: 1.0000e-04\n",
      "Epoch 11/40\n",
      "201/201 [==============================] - 173s 863ms/step - loss: 0.8866 - accuracy: 0.6840 - val_loss: 0.8401 - val_accuracy: 0.6943 - lr: 1.0000e-04\n",
      "Epoch 12/40\n",
      "201/201 [==============================] - 176s 876ms/step - loss: 0.8832 - accuracy: 0.6915 - val_loss: 0.8358 - val_accuracy: 0.7031 - lr: 9.0484e-05\n",
      "Epoch 13/40\n",
      "201/201 [==============================] - 173s 861ms/step - loss: 0.8827 - accuracy: 0.6890 - val_loss: 0.8315 - val_accuracy: 0.7031 - lr: 8.1873e-05\n",
      "Epoch 14/40\n",
      "201/201 [==============================] - 176s 874ms/step - loss: 0.8725 - accuracy: 0.6853 - val_loss: 0.8482 - val_accuracy: 0.7105 - lr: 7.4082e-05\n",
      "Epoch 15/40\n",
      "201/201 [==============================] - 174s 865ms/step - loss: 0.8766 - accuracy: 0.6875 - val_loss: 0.8453 - val_accuracy: 0.7099 - lr: 6.7032e-05\n",
      "Epoch 16/40\n",
      "201/201 [==============================] - 175s 869ms/step - loss: 0.8682 - accuracy: 0.6911 - val_loss: 0.8362 - val_accuracy: 0.7031 - lr: 6.0653e-05\n",
      "Epoch 17/40\n",
      "201/201 [==============================] - 177s 882ms/step - loss: 0.8639 - accuracy: 0.6918 - val_loss: 0.8388 - val_accuracy: 0.7018 - lr: 5.4881e-05\n",
      "Epoch 18/40\n",
      "201/201 [==============================] - 174s 867ms/step - loss: 0.8637 - accuracy: 0.6904 - val_loss: 0.8321 - val_accuracy: 0.7168 - lr: 4.9659e-05\n",
      "Epoch 19/40\n",
      "201/201 [==============================] - 175s 871ms/step - loss: 0.8627 - accuracy: 0.6950 - val_loss: 0.8415 - val_accuracy: 0.7031 - lr: 4.4933e-05\n",
      "Epoch 20/40\n",
      "201/201 [==============================] - 177s 879ms/step - loss: 0.8552 - accuracy: 0.6984 - val_loss: 0.8274 - val_accuracy: 0.7087 - lr: 4.0657e-05\n",
      "Epoch 21/40\n",
      "201/201 [==============================] - 175s 870ms/step - loss: 0.8628 - accuracy: 0.6906 - val_loss: 0.8455 - val_accuracy: 0.7155 - lr: 3.6788e-05\n",
      "Epoch 22/40\n",
      "201/201 [==============================] - 174s 865ms/step - loss: 0.8532 - accuracy: 0.6957 - val_loss: 0.8349 - val_accuracy: 0.7056 - lr: 3.3287e-05\n",
      "Epoch 23/40\n",
      "201/201 [==============================] - 174s 865ms/step - loss: 0.8542 - accuracy: 0.6967 - val_loss: 0.8308 - val_accuracy: 0.7068 - lr: 3.0119e-05\n",
      "Epoch 24/40\n",
      "201/201 [==============================] - 174s 868ms/step - loss: 0.8522 - accuracy: 0.6954 - val_loss: 0.8349 - val_accuracy: 0.7112 - lr: 2.7253e-05\n",
      "Epoch 25/40\n",
      "201/201 [==============================] - 176s 876ms/step - loss: 0.8572 - accuracy: 0.6953 - val_loss: 0.8327 - val_accuracy: 0.7162 - lr: 2.4660e-05\n",
      "Epoch 26/40\n",
      "201/201 [==============================] - 174s 867ms/step - loss: 0.8546 - accuracy: 0.6989 - val_loss: 0.8510 - val_accuracy: 0.7112 - lr: 2.2313e-05\n",
      "Epoch 27/40\n",
      "201/201 [==============================] - 174s 863ms/step - loss: 0.8486 - accuracy: 0.7001 - val_loss: 0.8319 - val_accuracy: 0.7180 - lr: 2.0190e-05\n",
      "Epoch 28/40\n",
      "201/201 [==============================] - 176s 877ms/step - loss: 0.8431 - accuracy: 0.7009 - val_loss: 0.8281 - val_accuracy: 0.7105 - lr: 1.8268e-05\n",
      "Epoch 29/40\n",
      "201/201 [==============================] - 174s 865ms/step - loss: 0.8398 - accuracy: 0.7028 - val_loss: 0.8263 - val_accuracy: 0.7205 - lr: 1.6530e-05\n",
      "Epoch 30/40\n",
      "201/201 [==============================] - 174s 867ms/step - loss: 0.8470 - accuracy: 0.6995 - val_loss: 0.8257 - val_accuracy: 0.7099 - lr: 1.4957e-05\n",
      "Epoch 31/40\n",
      "201/201 [==============================] - 174s 864ms/step - loss: 0.8423 - accuracy: 0.6995 - val_loss: 0.8226 - val_accuracy: 0.7187 - lr: 1.3534e-05\n",
      "Epoch 32/40\n",
      "201/201 [==============================] - 173s 859ms/step - loss: 0.8433 - accuracy: 0.7015 - val_loss: 0.8380 - val_accuracy: 0.7124 - lr: 1.2246e-05\n",
      "Epoch 33/40\n",
      "201/201 [==============================] - 172s 854ms/step - loss: 0.8463 - accuracy: 0.7001 - val_loss: 0.8213 - val_accuracy: 0.7218 - lr: 1.1080e-05\n",
      "Epoch 34/40\n",
      "201/201 [==============================] - 172s 855ms/step - loss: 0.8461 - accuracy: 0.7009 - val_loss: 0.8255 - val_accuracy: 0.7137 - lr: 1.0026e-05\n",
      "Epoch 35/40\n",
      "201/201 [==============================] - 172s 854ms/step - loss: 0.8401 - accuracy: 0.7049 - val_loss: 0.8273 - val_accuracy: 0.7143 - lr: 9.0718e-06\n",
      "Epoch 36/40\n",
      "201/201 [==============================] - 170s 848ms/step - loss: 0.8412 - accuracy: 0.7003 - val_loss: 0.8293 - val_accuracy: 0.7149 - lr: 8.2085e-06\n",
      "Epoch 37/40\n",
      "201/201 [==============================] - 170s 846ms/step - loss: 0.8371 - accuracy: 0.7053 - val_loss: 0.8200 - val_accuracy: 0.7205 - lr: 7.4274e-06\n",
      "Epoch 38/40\n",
      "201/201 [==============================] - 174s 864ms/step - loss: 0.8376 - accuracy: 0.7062 - val_loss: 0.8141 - val_accuracy: 0.7205 - lr: 6.7206e-06\n",
      "Epoch 39/40\n",
      "201/201 [==============================] - 170s 848ms/step - loss: 0.8392 - accuracy: 0.7032 - val_loss: 0.8139 - val_accuracy: 0.7187 - lr: 6.0810e-06\n",
      "Epoch 40/40\n",
      "201/201 [==============================] - 171s 853ms/step - loss: 0.8493 - accuracy: 0.6993 - val_loss: 0.8172 - val_accuracy: 0.7236 - lr: 5.5023e-06\n"
     ]
    }
   ],
   "source": [
    "# Train the model with the improved settings\n",
    "epochs = 40\n",
    "batch_size = 32\n",
    "\n",
    "history = model.fit(\n",
    "    train_datagen,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(val_images, val_labels),\n",
    "    callbacks=[early_stopping, lr_schedule]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b679ace4-4551-47d8-86ec-09eab25858e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/51 [==============================] - 25s 500ms/step - loss: 0.8172 - accuracy: 0.7236\n",
      "Validation Loss: 0.8172417879104614\n",
      "Validation Accuracy: 0.7236431837081909\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation set\n",
    "val_loss, val_accuracy = model.evaluate(val_images, val_labels, batch_size=batch_size)\n",
    "print(\"Validation Loss:\", val_loss)\n",
    "print(\"Validation Accuracy:\", val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c893daef-82cf-420e-b011-71b451770c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 32s 505ms/step - loss: 0.8269 - accuracy: 0.7084\n",
      "Test Loss: 0.8268979787826538\n",
      "Test Accuracy: 0.7084373235702515\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the testing set\n",
    "test_loss, test_accuracy = model.evaluate(test_images, test_labels, batch_size=batch_size)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "83ce0488-10a6-41d1-9c2d-280cd43e804a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 54). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: cnn_model-resnet_96x96/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: cnn_model-resnet_96x96/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('cnn_model-resnet_96x96')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c32f309-a273-4379-a287-1364080807f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m110",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m110"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
