{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cab858e-c9d3-4772-882c-b571b60ae4a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-27 05:02:16.043416: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-27 05:02:20.357509: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-08-27 05:02:20.359580: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-08-27 05:02:20.359606: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import ResNet50, ResNet152V2  # Import ResNet152V2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Flatten, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import classification_report\n",
    "from skimage.exposure import equalize_adapthist  # For histogram equalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd3090db-3b14-4526-a2aa-82508f547917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metadata CSV file\n",
    "metadata = pd.read_csv('HAM10000_metadata.csv')\n",
    "\n",
    "# Load images and labels\n",
    "image_folder = 'HAM10000_Images'\n",
    "image_size = (96, 96)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83e71bfb-5387-46bd-883a-65bb181fbe65",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "labels = []\n",
    "\n",
    "for index, row in metadata.iterrows():\n",
    "    image_path = os.path.join(image_folder, row['image_id'] + '.jpg')\n",
    "    image = imread(image_path)\n",
    "    resized_image = resize(image, image_size)\n",
    "    images.append(resized_image)\n",
    "    labels.append(row['dx'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "035df31d-ba2e-42a9-a1b7-560283d32ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.array(images)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef37e4bc-cc19-43bd-a105-a4641b335b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to categorical format\n",
    "label_mapping = {label: idx for idx, label in enumerate(np.unique(labels))}\n",
    "labels_encoded = np.array([label_mapping[label] for label in labels])\n",
    "labels_categorical = to_categorical(labels_encoded, num_classes=len(label_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1a39487-46f2-45eb-b153-5dbd5273f7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train, validation, and test sets\n",
    "train_images, test_images, train_labels, test_labels = train_test_split(\n",
    "    images, labels_categorical, test_size=0.2, random_state=42\n",
    ")\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(\n",
    "    train_images, train_labels, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c0503ab-91d8-4dc2-b20d-6c6150bc532e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images shape: (6409, 96, 96, 3)\n",
      "Train labels shape: (6409, 7)\n",
      "Validation images shape: (1603, 96, 96, 3)\n",
      "Validation labels shape: (1603, 7)\n",
      "Test images shape: (2003, 96, 96, 3)\n",
      "Test labels shape: (2003, 7)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print the shape of each set\n",
    "print(\"Train images shape:\", train_images.shape)\n",
    "print(\"Train labels shape:\", train_labels.shape)\n",
    "print(\"Validation images shape:\", val_images.shape)\n",
    "print(\"Validation labels shape:\", val_labels.shape)\n",
    "print(\"Test images shape:\", test_images.shape)\n",
    "print(\"Test labels shape:\", test_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "555f21f9-9b93-4cf8-8ac0-894740ed4344",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e15c46e7-69aa-40cc-80b4-6a459d0ad405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an ImageDataGenerator instance for data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    preprocessing_function=equalize_adapthist  # Apply histogram equalization\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12e3bd0c-ff53-4a97-8a26-f8146dc08efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply data augmentation only to the training set\n",
    "train_datagen = datagen.flow(train_images, train_labels, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f4b050-3137-4de3-a829-a9e4b0b444a5",
   "metadata": {},
   "source": [
    "ResNet152V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "33147dd7-6a12-43e0-8460-33e3006e70d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the pre-trained ResNet152V2 model\n",
    "base_model1 = ResNet152V2(include_top=False, weights='imagenet', input_shape=(96, 96, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fe666435-93ca-405f-903e-8ac2f35e43e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune the model\n",
    "# fine_tune_at = 555\n",
    "# for layer in base_model1.layers[:fine_tune_at]:\n",
    "#     layer.trainable = False\n",
    "for layer in base_model1.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a6df958a-f860-4cbf-8919-8e8be624dc70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resnet152v2 (Functional)    (None, 3, 3, 2048)        58331648  \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 18432)             0         \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 18432)            73728     \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 128)               2359424   \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 128)              512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 7)                 903       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 60,766,215\n",
      "Trainable params: 2,397,447\n",
      "Non-trainable params: 58,368,768\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Implement your specified model\n",
    "model1 = Sequential()\n",
    "model1.add(base_model1)\n",
    "model1.add(Flatten())\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Dense(128, activation='relu'))\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Dropout(0.5))\n",
    "model1.add(Dense(7, activation='softmax'))\n",
    "print(model1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bff71f0-a191-4004-8d6a-d635413ad002",
   "metadata": {},
   "source": [
    "EfficientNetB7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d661d4e-433d-4a03-aaef-78dd9c826194",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG19\n",
    "\n",
    "base_model3 = VGG19(include_top = False,input_shape=(96, 96, 3),weights='imagenet')\n",
    "# Fine-tune the model\n",
    "fine_tune_at = 20\n",
    "for layer in base_model3.layers[:fine_tune_at]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model3 = Sequential()\n",
    "model3.add(base_model2)\n",
    "model3.add(GlobalAveragePooling2D())\n",
    "model3.add(Dense(128, activation='relu'))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(Dropout(0.5))\n",
    "model3.add(Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3059099e-f67e-4325-aa9b-3f2f26d2bb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate scheduling\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1447f08-63eb-495c-82bc-0a60e109c477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca04263d-69fd-4125-938b-2bf651737977",
   "metadata": {},
   "source": [
    "Model 1 resnet152v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a4ab56aa-f8e7-43b0-994f-8f85ac67823b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compile the model\n",
    "model1.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ea7f4407-05ab-4071-8a7b-818099239c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "201/201 [==============================] - 348s 2s/step - loss: 2.1297 - accuracy: 0.3848 - val_loss: 1.7415 - val_accuracy: 0.5384 - lr: 1.0000e-04\n",
      "Epoch 2/40\n",
      "201/201 [==============================] - 339s 2s/step - loss: 1.8646 - accuracy: 0.4803 - val_loss: 1.5173 - val_accuracy: 0.6095 - lr: 1.0000e-04\n",
      "Epoch 3/40\n",
      "201/201 [==============================] - 340s 2s/step - loss: 1.6814 - accuracy: 0.5339 - val_loss: 1.4377 - val_accuracy: 0.6369 - lr: 1.0000e-04\n",
      "Epoch 4/40\n",
      "201/201 [==============================] - 341s 2s/step - loss: 1.6040 - accuracy: 0.5494 - val_loss: 1.2885 - val_accuracy: 0.6600 - lr: 1.0000e-04\n",
      "Epoch 5/40\n",
      "201/201 [==============================] - 342s 2s/step - loss: 1.4695 - accuracy: 0.5826 - val_loss: 1.2894 - val_accuracy: 0.6619 - lr: 1.0000e-04\n",
      "Epoch 6/40\n",
      "201/201 [==============================] - 338s 2s/step - loss: 1.3781 - accuracy: 0.6104 - val_loss: 1.2367 - val_accuracy: 0.6744 - lr: 1.0000e-04\n",
      "Epoch 7/40\n",
      "201/201 [==============================] - 341s 2s/step - loss: 1.3606 - accuracy: 0.6174 - val_loss: 1.1832 - val_accuracy: 0.6837 - lr: 1.0000e-04\n",
      "Epoch 8/40\n",
      "201/201 [==============================] - 337s 2s/step - loss: 1.2759 - accuracy: 0.6265 - val_loss: 1.1437 - val_accuracy: 0.6775 - lr: 1.0000e-04\n",
      "Epoch 9/40\n",
      "201/201 [==============================] - 347s 2s/step - loss: 1.2165 - accuracy: 0.6418 - val_loss: 1.1697 - val_accuracy: 0.6794 - lr: 1.0000e-04\n",
      "Epoch 10/40\n",
      "201/201 [==============================] - 341s 2s/step - loss: 1.1862 - accuracy: 0.6432 - val_loss: 1.1811 - val_accuracy: 0.6787 - lr: 1.0000e-04\n",
      "Epoch 11/40\n",
      "201/201 [==============================] - 337s 2s/step - loss: 1.1589 - accuracy: 0.6602 - val_loss: 1.1211 - val_accuracy: 0.6918 - lr: 1.0000e-04\n",
      "Epoch 12/40\n",
      "201/201 [==============================] - 337s 2s/step - loss: 1.1178 - accuracy: 0.6687 - val_loss: 1.1194 - val_accuracy: 0.6837 - lr: 1.0000e-04\n",
      "Epoch 13/40\n",
      "201/201 [==============================] - 336s 2s/step - loss: 1.0991 - accuracy: 0.6670 - val_loss: 1.1010 - val_accuracy: 0.6862 - lr: 1.0000e-04\n",
      "Epoch 14/40\n",
      "201/201 [==============================] - 337s 2s/step - loss: 1.0875 - accuracy: 0.6767 - val_loss: 1.0959 - val_accuracy: 0.6912 - lr: 1.0000e-04\n",
      "Epoch 15/40\n",
      "201/201 [==============================] - 337s 2s/step - loss: 1.0355 - accuracy: 0.6780 - val_loss: 1.0896 - val_accuracy: 0.6999 - lr: 1.0000e-04\n",
      "Epoch 16/40\n",
      "201/201 [==============================] - 339s 2s/step - loss: 1.0232 - accuracy: 0.6789 - val_loss: 1.0655 - val_accuracy: 0.6993 - lr: 1.0000e-04\n",
      "Epoch 17/40\n",
      "201/201 [==============================] - 336s 2s/step - loss: 1.0432 - accuracy: 0.6789 - val_loss: 1.0424 - val_accuracy: 0.7006 - lr: 1.0000e-04\n",
      "Epoch 18/40\n",
      "201/201 [==============================] - 336s 2s/step - loss: 0.9722 - accuracy: 0.6975 - val_loss: 1.0672 - val_accuracy: 0.6850 - lr: 1.0000e-04\n",
      "Epoch 19/40\n",
      "201/201 [==============================] - 338s 2s/step - loss: 0.9885 - accuracy: 0.6925 - val_loss: 1.0410 - val_accuracy: 0.6937 - lr: 1.0000e-04\n",
      "Epoch 20/40\n",
      "201/201 [==============================] - 337s 2s/step - loss: 0.9824 - accuracy: 0.6812 - val_loss: 1.0575 - val_accuracy: 0.6918 - lr: 1.0000e-04\n",
      "Epoch 21/40\n",
      "201/201 [==============================] - 341s 2s/step - loss: 0.9633 - accuracy: 0.6906 - val_loss: 1.0274 - val_accuracy: 0.6968 - lr: 1.0000e-04\n",
      "Epoch 22/40\n",
      "201/201 [==============================] - 336s 2s/step - loss: 0.9586 - accuracy: 0.6911 - val_loss: 1.0423 - val_accuracy: 0.6956 - lr: 1.0000e-04\n",
      "Epoch 23/40\n",
      "201/201 [==============================] - 339s 2s/step - loss: 0.9320 - accuracy: 0.6945 - val_loss: 1.0380 - val_accuracy: 0.6906 - lr: 1.0000e-04\n",
      "Epoch 24/40\n",
      "201/201 [==============================] - 336s 2s/step - loss: 0.9317 - accuracy: 0.6920 - val_loss: 1.0612 - val_accuracy: 0.6925 - lr: 1.0000e-04\n",
      "Epoch 25/40\n",
      "201/201 [==============================] - 340s 2s/step - loss: 0.9128 - accuracy: 0.6934 - val_loss: 1.0121 - val_accuracy: 0.6993 - lr: 1.0000e-04\n",
      "Epoch 26/40\n",
      "201/201 [==============================] - 342s 2s/step - loss: 0.9307 - accuracy: 0.6950 - val_loss: 1.0042 - val_accuracy: 0.6943 - lr: 1.0000e-04\n",
      "Epoch 27/40\n",
      "201/201 [==============================] - 338s 2s/step - loss: 0.8928 - accuracy: 0.7028 - val_loss: 0.9940 - val_accuracy: 0.6974 - lr: 1.0000e-04\n",
      "Epoch 28/40\n",
      "201/201 [==============================] - 340s 2s/step - loss: 0.8903 - accuracy: 0.7059 - val_loss: 1.0173 - val_accuracy: 0.6900 - lr: 1.0000e-04\n",
      "Epoch 29/40\n",
      "201/201 [==============================] - 339s 2s/step - loss: 0.8659 - accuracy: 0.6990 - val_loss: 0.9837 - val_accuracy: 0.7105 - lr: 1.0000e-04\n",
      "Epoch 30/40\n",
      "201/201 [==============================] - 337s 2s/step - loss: 0.8730 - accuracy: 0.7084 - val_loss: 0.9802 - val_accuracy: 0.7037 - lr: 1.0000e-04\n",
      "Epoch 31/40\n",
      "201/201 [==============================] - 344s 2s/step - loss: 0.8776 - accuracy: 0.6975 - val_loss: 0.9951 - val_accuracy: 0.7018 - lr: 1.0000e-04\n",
      "Epoch 32/40\n",
      "201/201 [==============================] - 338s 2s/step - loss: 0.8562 - accuracy: 0.7096 - val_loss: 0.9888 - val_accuracy: 0.7080 - lr: 1.0000e-04\n",
      "Epoch 33/40\n",
      "201/201 [==============================] - 337s 2s/step - loss: 0.8525 - accuracy: 0.7062 - val_loss: 0.9723 - val_accuracy: 0.7093 - lr: 1.0000e-04\n",
      "Epoch 34/40\n",
      "201/201 [==============================] - 338s 2s/step - loss: 0.8460 - accuracy: 0.7165 - val_loss: 0.9577 - val_accuracy: 0.7068 - lr: 1.0000e-04\n",
      "Epoch 35/40\n",
      "201/201 [==============================] - 338s 2s/step - loss: 0.8434 - accuracy: 0.7160 - val_loss: 0.9548 - val_accuracy: 0.7068 - lr: 1.0000e-04\n",
      "Epoch 36/40\n",
      "201/201 [==============================] - 339s 2s/step - loss: 0.8488 - accuracy: 0.7071 - val_loss: 0.9547 - val_accuracy: 0.7024 - lr: 1.0000e-04\n",
      "Epoch 37/40\n",
      "201/201 [==============================] - 342s 2s/step - loss: 0.8476 - accuracy: 0.7092 - val_loss: 0.9686 - val_accuracy: 0.7018 - lr: 1.0000e-04\n",
      "Epoch 38/40\n",
      "201/201 [==============================] - 338s 2s/step - loss: 0.8235 - accuracy: 0.7168 - val_loss: 0.9699 - val_accuracy: 0.6999 - lr: 1.0000e-04\n",
      "Epoch 39/40\n",
      "201/201 [==============================] - 337s 2s/step - loss: 0.8154 - accuracy: 0.7174 - val_loss: 0.9545 - val_accuracy: 0.6987 - lr: 1.0000e-04\n",
      "Epoch 40/40\n",
      "201/201 [==============================] - 336s 2s/step - loss: 0.8188 - accuracy: 0.7145 - val_loss: 0.9645 - val_accuracy: 0.7037 - lr: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "# Train the model with the improved settings\n",
    "epochs = 40\n",
    "batch_size = 32\n",
    "\n",
    "history = model1.fit(\n",
    "    train_datagen,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(val_images, val_labels),\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0e64e7df-ad9d-406e-9050-0b16bd46a85a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/51 [==============================] - 62s 1s/step - loss: 0.9645 - accuracy: 0.7037\n",
      "Validation Loss: 0.964523434638977\n",
      "Validation Accuracy: 0.7036805748939514\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation set\n",
    "val_loss, val_accuracy = model1.evaluate(val_images, val_labels, batch_size=batch_size)\n",
    "print(\"Validation Loss:\", val_loss)\n",
    "print(\"Validation Accuracy:\", val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b78c95fa-b246-4c5f-b603-c2d6480b1852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 81s 1s/step - loss: 0.9838 - accuracy: 0.7109\n",
      "Test Loss: 0.9837701320648193\n",
      "Test Accuracy: 0.7109336256980896\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the testing set\n",
    "test_loss, test_accuracy = model1.evaluate(test_images, test_labels, batch_size=batch_size)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "48212e0e-6b51-4d9c-b992-6da4305ad9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 80s 1s/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.06      0.10        69\n",
      "           1       0.49      0.28      0.36        93\n",
      "           2       0.50      0.26      0.34       228\n",
      "           3       0.00      0.00      0.00        28\n",
      "           4       0.48      0.22      0.30       226\n",
      "           5       0.75      0.96      0.84      1338\n",
      "           6       0.17      0.05      0.07        21\n",
      "\n",
      "    accuracy                           0.71      2003\n",
      "   macro avg       0.40      0.26      0.29      2003\n",
      "weighted avg       0.65      0.71      0.66      2003\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Generate a classification report\n",
    "y_pred = model1.predict(test_images)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "print(classification_report(np.argmax(test_labels, axis=1), y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "626e44a4-d87f-4881-8df2-5d81bcb74aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 156). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: cnn_model-resnet152v2_96x96v2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: cnn_model-resnet152v2_96x96v2/assets\n"
     ]
    }
   ],
   "source": [
    "# Save the model for later use in your web app\n",
    "model1.save('cnn_model-resnet152v2_96x96v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d375b638-9d84-47f6-867a-4961895a7951",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m110",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m110"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
